---
layout: post
title: math
categories:
- 科教
tags:
- 科教
---

策略 (Policy)是一个函数，输入当前的状态 s，输出采取动作 a 的概率 $\pi(s,a)$ .

一个策略 $\pi$ 下的一个状态s的价值v(s)定义：按照这个策略，从这个状态出发，系统能够获得的递减奖励期望:

$$ v(s)= E_{\pi}[\sum_{k=0}^\infty \gamma^k R_{k} ]= E_{\pi}[R_{0}+\gamma R_{1}+...]$$

$$ v(s)= \sum_{a \in A}\pi(s,a)( R_{s,a}+\gamma\sum_{s' \in S}T_{s,a}^{s'}v(s') )$$

将价值扩展到状态-动作对上。一个状态-动作对的价值定义如下所示。

$$q(s,a)=R(s,a)+ E_{\pi}[\sum_{k=1}^\infty \gamma^k R_{k} ]$$

$$\pi_{i+1}(s,a) =
\begin{cases}
1,  & a = argmax_{a}R_{s,a}+\gamma \sum_{s' \in S}T_{s,a}^{s'}v(s') \\[2ex]
0,  & a \neq argmax_{a}R_{s,a}+\gamma \sum_{s' \in S}T_{s,a}^{s'}v(s') 
\end{cases}$$

$$\pi_{i+1}(s,a) =$$
$$\begin{matrix} 
 1  & a = argmax_{a}R_{s,a}+\gamma \sum_{s' \in S}T_{s,a}^{s'}v(s') \\ 
 0  & a \neq argmax_{a}R_{s,a}+\gamma \sum_{s' \in S}T_{s,a}^{s'}v(s') \\
\end{matrix}$$


\begin{eqnarray} 
\end{eqnarray}

\begin{equation}
   |\psi_1\rangle = a|0\rangle + b|1\rangle
\end{equation}

$$
  \begin{align}
    |\psi_1\rangle &= a|0\rangle + b|1\rangle \\\\
    |\psi_2\rangle &= c|0\rangle + d|1\rangle
  \end{align}
$$


$$\begin{align*}
    |\psi_1\rangle &= a|0\rangle + b|1\rangle \\\\
    |\psi_2\rangle &= c|0\rangle + d|1\rangle
\end{align*}$$


Test a display math with equation number:

$$
\begin{align}
    |\psi_1\rangle &= a|0\rangle + b|1\rangle \\\\
    |\psi_2\rangle &= c|0\rangle + d|1\rangle
\end{align}
$$